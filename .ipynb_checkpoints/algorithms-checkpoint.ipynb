{
 "metadata": {
  "name": "",
  "signature": "sha256:6f1e74150f788f52730bec76a047860fc7014882c32d5ed08048ea8f3e2c03be"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
      "import numpy as np\n",
      "from sklearn import preprocessing\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.cluster import MiniBatchKMeans\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "import time\n",
      "from data_encoding import train_test\n",
      "from sklearn.svm import SVR\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "\n",
      "def label_encoder(data, binary_cols):\n",
      "    label_enc = LabelEncoder()\n",
      "\n",
      "    for col in binary_cols:\n",
      "        label_enc.fit(data[col])\n",
      "        data[col] = label_enc.transform(data[col])\n",
      "    encoded_binary = np.array(data[binary_cols])\n",
      "    return  encoded_binary\n",
      "\n",
      "\n",
      "def dummy_encoder(train_X,test_X,categorical_variable_list):\n",
      "    enc = OneHotEncoder(n_values ='auto',categorical_features=categorical_variable_list)\n",
      "    train_X = enc.fit_transform(train_X).toarray()\n",
      "    test_X = enc.transform(test_X).toarray()\n",
      "    return train_X, test_X\n",
      "\n",
      "\n",
      "\n",
      "def normalize(X):\n",
      "    normalizer = preprocessing.Normalizer().fit(X)\n",
      "    normalized_X = normalizer.transform(X)\n",
      "    return normalized_X\n",
      "\n",
      "\n",
      "def category_manipulation(train,test):\n",
      "    vect = CountVectorizer(tokenizer=lambda text: text.split(','))\n",
      "    \n",
      "    cat_fea = vect.fit_transform(train['categories'].fillna(''))\n",
      "    cat_fea = cat_fea.todense()\n",
      "    idx_max_1 = cat_fea > 1\n",
      "    cat_fea[idx_max_1] = 1\n",
      "    \n",
      "    cat_fea_test = vect.transform(test['categories'].fillna(''))\n",
      "    cat_fea_test = cat_fea_test.todense()\n",
      "    idx_max_1 = cat_fea_test > 1\n",
      "    cat_fea_test[idx_max_1] = 1\n",
      "    \n",
      "    # CATEGORY CLUSTERS\n",
      "    #  Based on the category extracted before, the idea is to create a n clusters to\n",
      "    #  aggregate set of similar categories\n",
      "    for esti in (20):#,35,50):#,60,70,80,90,100,110,125):\n",
      "        km = MiniBatchKMeans(n_clusters=esti, random_state=888)#, init_size=esti*10)\n",
      "    #       init='k-means++', n_clusters=3, n_init=10\n",
      "        print \"fitting \"+str(esti)+\" clusters - category\"\n",
      "        init_time = time.time()\n",
      "        km.fit(cat_fea)\n",
      "        print (time.time()-init_time)/60\n",
      "\n",
      "        train['cat_clust_'+str(esti)] = km.predict(cat_fea)\n",
      "        test['cat_clust_'+str(esti)] = km.predict(cat_fea_test)\n",
      "    train['cat_clust_100'] = km.predict(cat_fea)\n",
      "    test['cat_clust_100'] = km.predict(cat_fea_test)\n",
      "    return train,test\n",
      "\n",
      "\n",
      "def train_test():\n",
      "\n",
      "    train = pd.read_csv('train_new.csv', header=0)\n",
      "    pred = pd.read_csv('test_new.csv', header=0)\n",
      "    \n",
      "    train, pred = category_manipulation(train, pred)\n",
      "\n",
      "    # print train.columns.values\n",
      "    review_id = pred['review_id']\n",
      "    \n",
      "\n",
      "    del train[\"business_id\"], train[\"date\"], train[\"review_id\"], train[\"text\"], train[\"type_x\"], train[\"user_id\"],\\\n",
      "        train[\"votes_cool_x\"], train[\"votes_funny_x\"],train[\"full_address\"], train[\"latitude\"], train[\"longitude\"],\\\n",
      "        train[\"name_x\"], train[\"neighborhoods\"], train[\"type_y\"], train[\"name_y\"], train[\"type\"], train[\"votes_cool_y\"],\\\n",
      "        train[\"votes_funny_y\"], train[\"votes_useful_y\"], train[\"state\"], train[\"city\"],train[\"categories\"]\n",
      "\n",
      "    #print test.columns.values\n",
      "    del pred[\"business_id\"], pred[\"date\"], pred[\"review_id\"], pred[\"text\"], pred[\"type_x\"], pred[\"user_id\"],\\\n",
      "        pred[\"full_address\"], pred[\"latitude\"], pred[\"longitude\"],\\\n",
      "        pred[\"name_x\"], pred[\"neighborhoods\"], pred[\"type_y\"], pred[\"name_y\"], pred[\"type\"], pred[\"state\"], pred[\"city\"],pred[\"categories\"]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    ### Vectorizing  ZIP\n",
      "    vect = CountVectorizer(tokenizer=lambda text: text.split(','))\n",
      "    zip_fea = vect.fit_transform(train['zip_code'])#.fillna(0)\n",
      "    # print zip_fea\n",
      "    zip_fea = zip_fea.todense()\n",
      "    # print zip_fea\n",
      "    idx_max_1 = zip_fea > 1\n",
      "    zip_fea[idx_max_1] = 1\n",
      "    zip_fea_test = vect.transform(pred['zip_code'])\n",
      "    zip_fea_test = zip_fea_test.todense()\n",
      "    idx_max_1 = zip_fea_test > 1\n",
      "    zip_fea_test[idx_max_1] = 1\n",
      "\n",
      "\n",
      "    km = MiniBatchKMeans(n_clusters=100, random_state=1377, init_size=100*10)\n",
      "\n",
      "    km.fit(zip_fea)\n",
      "    train['zip_clust'] = km.predict(zip_fea)\n",
      "    pred['zip_clust'] = km.predict(zip_fea_test)\n",
      "\n",
      "    #deleting actual zip codes\n",
      "    del train['zip_code'], pred['zip_code']\n",
      "\n",
      "\n",
      "\n",
      "    # print train.head()\n",
      "    # print pred.head()\n",
      "\n",
      "    target = np.array(train[\"votes_useful_x\"])\n",
      "    target_var = [\"votes_useful_x\"]\n",
      "    binary_var = [\"open\"]\n",
      "    categorical_var = [\"cat_clust_100\", \"zip_clust\", \"user_id_cluster_125\"]\n",
      "\n",
      "\n",
      "    col_names_train= train.columns.values\n",
      "    col_names_pred = pred.columns.values\n",
      "\n",
      "    numerical_var = [col for col in col_names_train if col not in binary_var if col not in categorical_var if col not in target_var]\n",
      "    numerical_var_pred = [col for col in col_names_pred if col not in binary_var if col not in categorical_var]\n",
      "    # encoded_categorical, encoded_binary = label_encoder(train, binary_var, categorical_var)\n",
      "    # encoded_categorical_pred, encoded_binary_pred= label_encoder(pred, binary_var, categorical_var)\n",
      "    encoded_binary = label_encoder(train, binary_var)\n",
      "    encoded_binary_pred= label_encoder(pred, binary_var)\n",
      "\n",
      "    categorical_variables = np.array(train[categorical_var])\n",
      "    categorical_variables_pred = np.array(pred[categorical_var])\n",
      "    numerical_data = np.array(train[numerical_var])\n",
      "    numerical_data_pred = np.array(pred[numerical_var_pred])\n",
      "\n",
      "\n",
      "    training_data_transformed = np.concatenate((categorical_variables,encoded_binary,numerical_data),axis=1)\n",
      "    pred_data_transformed = np.concatenate((categorical_variables_pred,encoded_binary_pred,numerical_data_pred),axis=1)\n",
      "\n",
      "\n",
      "\n",
      "    train_x = training_data_transformed\n",
      "    train_y = target\n",
      "    pred_x = pred_data_transformed\n",
      "\n",
      "    #pd.DataFrame(pred_x).to_csv(\"test_labelenc.csv\")\n",
      "\n",
      "    #REMEMBER TO ENCODE THE CATEGORICAL VARS\n",
      "    # print train_x[1:5],\n",
      "    train_x, pred_x = dummy_encoder(train_x, pred_x, categorical_variable_list = list(range(0,1,1)))\n",
      "    # print train_x[0:5,], test_x[0:5,]\n",
      "\n",
      "    #normalizing (if required)\n",
      "    train_x_norm = normalize(train_x)\n",
      "    #test_x_norm = normalize(test_x)\n",
      "    pred_x_norm = normalize(pred_x)\n",
      "\n",
      "\n",
      "    return train_x, train_y,train_x_norm, pred_x, pred_x_norm, review_id\n",
      "\n",
      "\n",
      "# train_test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_x, train_y,train_x_norm, pred_x, pred_x_norm, review_id = train_test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "fitting 20 clusters - category\n",
        "0.0327848156293"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "fitting 35 clusters - category"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.0710525830587"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "fitting 50 clusters - category"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.0959849476814"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "AttributeError",
       "evalue": "fillna not found",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-5-7fee844b02e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_x_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_x_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-3-bac91f567001>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m### Vectorizing  ZIP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mzip_fea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zip_code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;31m# print zip_fea\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mzip_fea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_fea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/RashmiSaurabh/anaconda/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: fillna not found"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "def rsmle_(predicted,actual):\n",
      "   return np.sqrt(np.mean((pow(np.log(predicted+1) - np.log(actual+1),2))))\n",
      "\n",
      "def support_vector_regressor(X_train, y_train):\n",
      "    clf = SVR()\n",
      "    clf.fit(X_train, y_train)\n",
      "    # plot_learning_curve(clf, title =\"Support Vector learning curve\", X = X_train,y = y_train, ylim=(0, 1.1))\n",
      "    #y_pred_SVC = clf.predict(X_train)\n",
      "    #print clf.predict(X_train)\n",
      "    pd.DataFrame(clf.predict(X_train)).to_csv(\"y_pred_svr.csv\")\n",
      "    print np.sqrt(np.mean((pow(np.log(clf.predict(X_train)+1) - np.log(y_train+1),2))))\n",
      "    # plot_validation_curve(clf,title =\"Support Vector Classifier validation curve\", X=X_train, y=y_train,param_name='C', param_range = [1, 5, 20, 50])\n",
      "    # print y_train, y_pred_SVC\n",
      "    # print clf.score(X_train, y_train)\n",
      "    # # cross_validation_accuracy= cross_val_score(clf, X_train, y_train, cv = 5, scoring = 'accuracy').mean()\n",
      "    # print \"Support Vector Classifier : Training set metrics\"\n",
      "    # print \"Cross validation accuracy:\", cross_validation_accuracy\n",
      "    # print_metrics(y_train, y_pred_SVC)\n",
      "    # file = open(\"SVM_clf.p\", \"wb\")\n",
      "    # pickle.dump(clf, file)\n",
      "    # file.close()\n",
      "\n",
      "if __name__ == '__main__':\n",
      "\n",
      "    train_x, train_y,train_x_norm, pred_x, pred_x_norm, review_id = train_test()\n",
      "\n",
      "    print \"data fetched...\"\n",
      "\n",
      "    rf = RandomForestRegressor()\n",
      "    rf.fit(train_x, train_y)\n",
      "    # print rf.score(train_x, train_y)\n",
      "    Votes = rf.predict(pred_x)\n",
      "    Id = np.array(review_id)\n",
      "    print len(Votes), len(Id)\n",
      "    df = pd.DataFrame(Votes,Id)\n",
      "    df.to_csv(\"submission_rf.csv\", engine=\"python\")\n",
      "    print \"rf done\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    # list_data = [Id,Votes]\n",
      "    # submission_rf = pd.concat(list_data)\n",
      "    # submission_rf= np.concatenate((Id,Votes),axis=0)\n",
      "    # print submission_rf\n",
      "    # # np.savetxt(\"rf_predicted.csv\", submission_rf, delimiter=',',fmt=\"%s\")\n",
      "    # # pd.DataFrame(bus_id,rf.predict(pred_x)).to_csv(\"rf_predicted.csv\")\n",
      "\n",
      "\n",
      "    # print rf.score(train_x,train_y)\n",
      "    # print rf.score(test_x,test_y)\n",
      "    # print \"RMSE rf:\",rsmle_(rf.predict(pred_x))\n",
      "    #print train_x[1:5,], train_y[1:5], test_x[1:5,],test_y[1:5]\n",
      "    # support_vector_regressor(train_x_norm,train_y)\n",
      "\n",
      "    # gbr = GradientBoostingRegressor()\n",
      "    # gbr.fit(train_x, train_y)\n",
      "    # print gbr.score(train_x, train_y)\n",
      "    # Votes = pd.DataFrame(gbr.predict(pred_x))\n",
      "    # Id = bus_id\n",
      "    # print pd.concat(Id,Votes)\n",
      "    # pd.concat(Id,Votes).to_csv(\"gbr_predicted.csv\")\n",
      "    # print \"gbr done\"\n",
      "\n",
      "    # print \"RMSE gbr:\", rsmle_(gbr.predict(test_x),test_y)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}